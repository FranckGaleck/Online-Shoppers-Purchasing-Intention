# -*- coding: utf-8 -*-
"""MFG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gsxsXx3_sY3rdLNsZ6w-znKRw-R5pOvy

# 1. Introduction

This notebook aims to predict online shopper purchasing intention using various machine learning models. The dataset comes from the UCI Machine Learning Repository and contains behavioral and technical features for over 12,000 sessions.

#2. Data Loading and Initial Exploration
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Display settings
pd.set_option('display.max_columns', None)
sns.set(style='whitegrid')

#loading the dataset
csv = "online_shoppers_intention.csv"
df = pd.read_csv(csv)

# print the first row
df.head()

"""# Initial Dataset Check (omitted in final report)
The dataset was verified for missing values and data types using `df.info()` and `df.describe()`, but this output is not included here for brevity.
"""

#checking the distribution of the target variable
df['Revenue'].value_counts(normalize=True)
#We can see they have unbalanced data, 0.845 False for 0.155 True

"""#Preprocessing
The first step in our preprocessing pipeline is to ensure that all variables used for modeling
are in a numerical format. This is a critical requirement for most machine learning algorithms,
especially tree-based models such as Random Forest and XGBoost, which do not natively handle
categorical or boolean features.

We start by:
- Mapping the 'Month' feature to its corresponding numerical value (Jan=1, ..., Dec=12).
- Mapping the 'VisitorType' feature to integers (Returning_Visitor=2, New_Visitor=1, Other=0).
- Converting boolean columns such as 'Revenue' and 'Weekend' into integers (True → 1, False → 0).

These transformations ensure that all input features are numerical and ready to be fed into ML models.

### Mapping categorical and boolean features to numerical values
"""

# Define a mapping from month and visitor to numeric order
month_map = {
    'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4,
    'May': 5, 'June': 6, 'Jul': 7, 'Aug': 8,
    'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12
}

visitor_map = {
    'Returning_Visitor': 0,
    'New_Visitor': 1,
    'Other': 2
}


# Apply the mapping
df_processed = df.copy()
#print(df_processed.columns)
df_processed['Month'] = df_processed['Month'].map(month_map)
df_processed['VisitorType'] = df['VisitorType'].map(visitor_map)
# Convert boolean columns to integers
df_processed['Revenue'] = df_processed['Revenue'].astype(int)
df_processed['Weekend'] = df_processed['Weekend'].astype(int)
# Check result
#df_processed[['Month']].head()
#df_processed.head()
#df_processed.info()

"""### Visualizing the impact of PageValues on Revenue"""

# Boxplot for numerical variable
sns.boxplot(x='Revenue', y='PageValues', data=df)
plt.title('PageValues vs Revenue')
plt.show()

"""### Correlation analysis: filtering only features with |corr| ≥ 0.05 with Revenue

"""

# Compute correlation matrix
corr_matrix = df_processed.corr(numeric_only=True)

# Filter columns that have correlation ≥ 0.05 (absolute value) with 'Revenue'
target_corr = corr_matrix['Revenue'].abs()
cols_to_keep = target_corr[target_corr >= 0.05].index

# Filter the full matrix to keep only those columns/rows
filtered_corr = corr_matrix.loc[cols_to_keep, cols_to_keep]

# Plot the filtered heatmap
plt.figure(figsize=(12, 8))  # Adjust figure size
sns.heatmap(filtered_corr, annot=True, fmt=".2f", cmap="viridis", square=True, annot_kws={"size": 12})  # Increase font size and change colormap
plt.title('Filtered Correlation Matrix (|corr with Revenue| ≥ 0.05)')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

"""### Train/Test Split — Full Feature Set

We start by defining our `X` and `y` from the full preprocessed dataset. This includes all 17 numerical features.  
We then split the data into training and test sets (80/20), stratifying on the target variable to preserve class imbalance in both subsets.
"""

from sklearn.model_selection import train_test_split
# Full feature set
X = df_processed.drop(columns=['Revenue'])
y = df_processed['Revenue']

# Check shape and class distribution
print("Original feature set")
print("X shape:", X.shape)
print("y distribution:\n", y.value_counts(normalize=True))

# Train/Test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.2, random_state=42
)

"""### Train/Test Split — Selected Features

To evaluate whether a reduced set of informative features can yield similar performance, we repeat the same process but only using a subset of 10 features selected through correlation filtering and multicollinearity analysis.

"""

# Selected features based on correlation and collinearity analysis
selected_features = [
    'Administrative',
    'Administrative_Duration',
    'Informational',
    'Informational_Duration',
    'ProductRelated',
    'BounceRates',
    'PageValues',
    'SpecialDay',
    'Month',
    'VisitorType'
]

X_f = df_processed[selected_features]
y_f = df_processed['Revenue']

# Check shape and class distribution
print("Selected feature set")
print("X_f shape:", X_f.shape)
print("y_f distribution:\n", y_f.value_counts(normalize=True))

# Train/Test split
X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(
    X_f, y_f, stratify=y_f, test_size=0.2, random_state=42
)

"""#Logistic Regresssion

##Logistic Regression – With Hyperparameter Tuning
We first train a Logistic Regression model with a grid search to find the best hyperparameters. The search is conducted using 5-fold cross-validation and optimizes for recall, which is critical for minimizing false negatives (i.e., missed buyers).
"""

from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, recall_score, f1_score
import numpy as np

# Pipeline: standardization + logistic regression
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('clf', LogisticRegression(max_iter=500))
])

# Hyperparameter grid for logistic regression
param_grid = {
    'clf__C': [0.01, 0.1, 1, 10, 100],
    'clf__solver': ['lbfgs', 'liblinear'],
    'clf__penalty': ['l2'],  # 'l1' works only with 'liblinear'
    'clf__class_weight': [
        'balanced',           # built-in way to handle imbalanced data
        {0: 1, 1: 2},          # custom weight for positive class
        {0: 1, 1: 3}
    ]
}

# Use StratifiedKFold to maintain class distribution in each fold
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Perform grid search to maximize recall (important due to imbalance)
grid = GridSearchCV(
    pipeline,
    param_grid,
    scoring='recall',
    cv=cv,
    n_jobs=-1,
    verbose=1
)

# Fit on training data
grid.fit(X_train, y_train)

# Best model info
print("\nBest parameters:", grid.best_params_)
print(f"Best recall (CV - from grid search): {grid.best_score_:.3f}")

# Additional: Re-evaluate recall on best estimator with CV
cv_scores = cross_val_score(grid.best_estimator_, X_train, y_train, cv=cv, scoring='recall')
print(f"Recall CV scores: {np.round(cv_scores, 3)}")
print(f"Mean recall (CV): {cv_scores.mean():.3f} ± {cv_scores.std():.3f}")

# Evaluate best model on test set
y_pred = grid.predict(X_test)
test_recall = recall_score(y_test, y_pred)
test_f1 = f1_score(y_test, y_pred)

print(f"\nRecall (Test set): {test_recall:.3f}")
print(f"F1-score (Test set): {test_f1:.3f}")
print("\nFull classification report:")
print(classification_report(y_test, y_pred))

"""##Logistic Regression – With Feature Selection
We now apply the same pipeline on a reduced feature set (selected via correlation analysis). This allows us to evaluate whether compact models can achieve similar performance while reducing training time.
"""

pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('clf', LogisticRegression(max_iter=1000))
])

param_grid = {
    'clf__C': [0.01, 0.1, 1, 10],
    'clf__penalty': ['l2'],
    'clf__solver': ['lbfgs', 'liblinear'],
    'clf__class_weight': ['balanced', {0: 1, 1: 2}, {0: 1, 1: 3}]
}

# Grid search with stratified CV
grid = GridSearchCV(pipe, param_grid, scoring='recall', cv=cv, n_jobs=-1, verbose=1)
grid.fit(X_train_f, y_train_f)

#Best parameters
print("Best parameters:", grid.best_params_)
print(f"Best recall (CV - from grid search): {grid.best_score_:.3f}")

#Cross-validation scores on best estimator
cv_scores = cross_val_score(grid.best_estimator_, X_train_f, y_train_f, cv=cv, scoring='recall')
print("Recall CV scores:", np.round(cv_scores, 3))
print(f"Mean recall (CV): {cv_scores.mean():.3f} ± {cv_scores.std():.3f}")

#Evaluate on test set
y_pred_f = grid.predict(X_test_f)
test_recall = recall_score(y_test_f, y_pred_f)
test_f1 = f1_score(y_test_f, y_pred_f)

print(f"\nRecall (Test set): {test_recall:.3f}")
print(f" F1-score (Test set): {test_f1:.3f}")

#Full report
print("\nFull classification report:")
print(classification_report(y_test_f, y_pred_f))

"""##Logistic Regression – With SMOTE Oversampling
Finally, we test whether SMOTE (Synthetic Minority Oversampling Technique) can improve performance by generating synthetic minority samples. We compare the F1-score to that of the class-weighted model.
"""

from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE
# Pipeline: SMOTE + scaling + logistic regression
pipeline = ImbPipeline([
    ('smote', SMOTE(random_state=42)),
    ('scaler', StandardScaler()),
    ('clf', LogisticRegression(max_iter=500))
])

# Hyperparameter grid
param_grid = {
    'clf__C': [0.01, 0.1, 1, 10],
    'clf__solver': ['lbfgs', 'liblinear'],
    'clf__penalty': ['l2'],
    'clf__class_weight': ['balanced', {0: 1, 1: 2}, {0: 1, 1: 3}]
}

# GridSearchCV setup
grid = GridSearchCV(
    estimator=pipeline,
    param_grid=param_grid,
    scoring='recall',
    cv=cv,
    n_jobs=-1,
    verbose=1
)

# Fit on training data
grid.fit(X_train, y_train)

# Display best parameters and CV recall
print("Best parameters:", grid.best_params_)
print(f"Best recall (CV - from grid search): {grid.best_score_:.3f}")

# Additional CV scores on best model
cv_scores = cross_val_score(grid.best_estimator_, X_train, y_train, cv=cv, scoring='recall')
print("Recall CV scores:", np.round(cv_scores, 3))
print(f"Mean recall (CV): {cv_scores.mean():.3f} ± {cv_scores.std():.3f}")

# Evaluate on test set
y_pred = grid.predict(X_test)
test_recall = recall_score(y_test, y_pred)
test_f1 = f1_score(y_test, y_pred)

print(f"\nRecall (Test set): {test_recall:.3f}")
print(f"F1-score (Test set): {test_f1:.3f}")

# Full classification report
print("\nClassification Report on Test Set:")
print(classification_report(y_test, y_pred))

"""#Random Forest

##Random Forest - Baseline (Full Features)
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report

# Base model
rf = RandomForestClassifier(random_state=42)

# Hyperparameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'class_weight': ['balanced']
}

# Grid search
grid_rf = GridSearchCV(
    estimator=rf,
    param_grid=param_grid,
    scoring='recall',
    cv=cv,
    n_jobs=-1,
    verbose=1
)

# Train the model
grid_rf.fit(X_train, y_train)

# Best parameters and recall from grid search
print("Best parameters:", grid_rf.best_params_)
print(f"Best recall (CV - from grid search): {grid_rf.best_score_:.3f}")

# Cross-validation recall on best estimator
cv_scores_rf = cross_val_score(grid_rf.best_estimator_, X_train, y_train, cv=cv, scoring='recall')
print("Recall CV scores:", np.round(cv_scores_rf, 3))
print(f"Mean recall (CV): {cv_scores_rf.mean():.3f} ± {cv_scores_rf.std():.3f}")

# Evaluate on test set
y_pred_rf = grid_rf.predict(X_test)
test_recall_rf = recall_score(y_test, y_pred_rf)
test_f1_rf = f1_score(y_test, y_pred_rf)

print(f"\nRecall (Test set): {test_recall_rf:.3f}")
print(f"F1-score (Test set): {test_f1_rf:.3f}")

# Full classification report
print("\nClassification Report on Test Set:")
print(classification_report(y_test, y_pred_rf))

"""#With feature selection"""

# Base model with feature selection
rf_f = RandomForestClassifier(random_state=42)

# Grid search
grid_fs = GridSearchCV(
    estimator=rf_f,
    param_grid=param_grid,
    scoring='recall',
    cv=cv,
    n_jobs=-1,
    verbose=1
)

# Fit on reduced feature set
grid_fs.fit(X_train_f, y_train_f)

# Best grid search result
print("Best parameters:", grid_fs.best_params_)
print(f"Best recall (CV - from grid search): {grid_fs.best_score_:.3f}")

# Cross-validation recall scores on best estimator
cv_scores_fs = cross_val_score(grid_fs.best_estimator_, X_train_f, y_train_f, cv=cv, scoring='recall')
print("Recall CV scores:", np.round(cv_scores_fs, 3))
print(f"Mean recall (CV): {cv_scores_fs.mean():.3f} ± {cv_scores_fs.std():.3f}")

# Evaluation on test set
y_pred_fs = grid_fs.predict(X_test_f)
test_recall_fs = recall_score(y_test_f, y_pred_fs)
test_f1_fs = f1_score(y_test_f, y_pred_fs)

print(f"\nRecall (Test set): {test_recall_fs:.3f}")
print(f"F1-score (Test set): {test_f1_fs:.3f}")

# Full classification report
print("\nClassification Report on Test Set:")
print(classification_report(y_test_f, y_pred_fs))

"""#With smote"""

# Define pipeline with SMOTE and Random Forest
pipeline_rf_smote = ImbPipeline([
    ('smote', SMOTE(random_state=42)),
    ('scaler', StandardScaler()),
    ('clf', RandomForestClassifier(random_state=42))
])

# Hyperparameter grid
param_grid_smote = {
    'clf__n_estimators': [100, 200],
    'clf__max_depth': [5, 10],
    'clf__min_samples_split': [2, 5, 10],
    'clf__class_weight': ['balanced']
}

# Stratified cross-validation setup
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Grid search
grid_rf_smote = GridSearchCV(
    pipeline_rf_smote,
    param_grid=param_grid_smote,
    scoring='recall',
    cv=cv,
    n_jobs=-1,
    verbose=1
)

# Train
grid_rf_smote.fit(X_train, y_train)

# Best results
print("Best parameters:", grid_rf_smote.best_params_)
print(f"Best recall (CV - from grid search): {grid_rf_smote.best_score_:.3f}")

# Cross-validation recall scores on best estimator
cv_scores_smote = cross_val_score(grid_rf_smote.best_estimator_, X_train, y_train, cv=cv, scoring='recall')
print("Recall CV scores:", np.round(cv_scores_smote, 3))
print(f"Mean recall (CV): {cv_scores_smote.mean():.3f} ± {cv_scores_smote.std():.3f}")

# Test set evaluation
y_pred_smote = grid_rf_smote.predict(X_test)
test_recall_smote = recall_score(y_test, y_pred_smote)
test_f1_smote = f1_score(y_test, y_pred_smote)

print(f"\nRecall (Test set): {test_recall_smote:.3f}")
print(f"F1-score (Test set): {test_f1_smote:.3f}")

# Classification report
print("\nClassification Report on Test Set:")
print(classification_report(y_test, y_pred_smote))

"""##Feature Importance"""

import pandas as pd
import matplotlib.pyplot as plt

importances_rf = grid_rf.best_estimator_.feature_importances_
features = X_train.columns
rf_importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': importances_rf
}).sort_values(by='Importance', ascending=True)

plt.figure(figsize=(8, 6))
plt.barh(rf_importance_df['Feature'], rf_importance_df['Importance'])
plt.title('Feature Importance (Random Forest)')
plt.tight_layout()
plt.show()

"""#XG boost



"""

from xgboost import XGBClassifier

# Handle class imbalance using scale_pos_weight
scale_pos_weight = y_train.value_counts()[0] / y_train.value_counts()[1]

# Base XGBoost model
xgb = XGBClassifier(
    use_label_encoder=False,
    eval_metric='logloss',
    scale_pos_weight=scale_pos_weight,
    random_state=42,
    n_jobs=-1
)

# Hyperparameter grid
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1],
    'subsample': [0.8, 1.0]
}

# Grid Search with recall as scoring metric
grid_xgb = GridSearchCV(
    estimator=xgb,
    param_grid=param_grid,
    scoring='recall',
    cv=cv,  # Stratified CV defined earlier
    verbose=1,
    n_jobs=-1
)

# Fit on training data
grid_xgb.fit(X_train, y_train)

# Best result
print("Best parameters:", grid_xgb.best_params_)
print(f"Best recall (CV - from grid search): {grid_xgb.best_score_:.3f}")

# Cross-validation recall scores on best estimator
cv_scores_xgb = cross_val_score(grid_xgb.best_estimator_, X_train, y_train, cv=cv, scoring='recall')
print("Recall CV scores:", np.round(cv_scores_xgb, 3))
print(f"Mean recall (CV): {cv_scores_xgb.mean():.3f} ± {cv_scores_xgb.std():.3f}")

# Test set evaluation
best_xgb = grid_xgb.best_estimator_
y_pred_xgb = best_xgb.predict(X_test)

test_recall_xgb = recall_score(y_test, y_pred_xgb)
test_f1_xgb = f1_score(y_test, y_pred_xgb)

print(f"\nRecall (Test set): {test_recall_xgb:.3f}")
print(f"F1-score (Test set): {test_f1_xgb:.3f}")

# Classification report
print("\nXGBoost - Classification Report on Test Set:")
print(classification_report(y_test, y_pred_xgb))

"""#Feature Importance"""

importances_xgb = best_xgb.feature_importances_
xgb_importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': importances_xgb
}).sort_values(by='Importance', ascending=True)

plt.figure(figsize=(8, 6))
plt.barh(xgb_importance_df['Feature'], xgb_importance_df['Importance'])
plt.title('Feature Importance (XGBoost)')
plt.tight_layout()
plt.show()

"""#With feature selection"""

# Recalculate imbalance ratio on filtered data
scale_pos_weight_f = y_train_f.value_counts()[0] / y_train_f.value_counts()[1]

# Same base model adapted to the filtered dataset
xgb_f = XGBClassifier(
    use_label_encoder=False,
    eval_metric='logloss',
    scale_pos_weight=scale_pos_weight_f,
    random_state=42,
    n_jobs=-1
)

# Reuse the same hyperparameter grid
grid_xgb_f = GridSearchCV(
    estimator=xgb_f,
    param_grid=param_grid,
    scoring='recall',
    cv=cv,
    verbose=1,
    n_jobs=-1
)

# Fit on filtered training set
grid_xgb_f.fit(X_train_f, y_train_f)

# Results
print("Best parameters:", grid_xgb_f.best_params_)
print(f"Best recall (CV - from grid search): {grid_xgb_f.best_score_:.3f}")

# Cross-validation recall scores on best estimator
cv_scores_xgb_f = cross_val_score(grid_xgb_f.best_estimator_, X_train_f, y_train_f, cv=cv, scoring='recall')
print("Recall CV scores:", np.round(cv_scores_xgb_f, 3))
print(f"Mean recall (CV): {cv_scores_xgb_f.mean():.3f} ± {cv_scores_xgb_f.std():.3f}")

# Test set evaluation
best_xgb_f = grid_xgb_f.best_estimator_
y_pred_xgb_f = best_xgb_f.predict(X_test_f)

test_recall_xgb_f = recall_score(y_test_f, y_pred_xgb_f)
test_f1_xgb_f = f1_score(y_test_f, y_pred_xgb_f)

print(f"\nRecall (Test set): {test_recall_xgb_f:.3f}")
print(f"F1-score (Test set): {test_f1_xgb_f:.3f}")

# Classification report
print("\nXGBoost (selected features) - Classification Report on Test Set:")
print(classification_report(y_test_f, y_pred_xgb_f))

from sklearn.metrics import precision_recall_curve, average_precision_score
import matplotlib.pyplot as plt

models = {
    'Logistic Regression': grid.best_estimator_,
    'Random Forest': grid_rf.best_estimator_,
    'XGBoost': grid_xgb.best_estimator_
}

plt.figure(figsize=(8, 6))

for name, model in models.items():
    if hasattr(model, "predict_proba"):
        y_scores = model.predict_proba(X_test)[:, 1]

    precision, recall, _ = precision_recall_curve(y_test, y_scores)
    ap = average_precision_score(y_test, y_scores)
    plt.plot(recall, precision, label=f'{name} (AP={ap:.2f})')

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

!pip install tabpfn

from tabpfn import TabPFNClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, recall_score, f1_score

# Train/test split (50/50 comme les autres tests pour TabPFN)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.5, stratify=y, random_state=42
)

# Initialisation et entraînement du modèle TabPFN
model = TabPFNClassifier()
model.fit(X_train, y_train)

# Prédictions sur le jeu de test
y_pred = model.predict(X_test)

# Évaluation finale
test_recall_tabpfn = recall_score(y_test, y_pred)
test_f1_tabpfn = f1_score(y_test, y_pred)

print(f"Recall (Test set): {test_recall_tabpfn:.3f}")
print(f"F1-score (Test set): {test_f1_tabpfn:.3f}")

# Rapport complet
print("\nTabPFN - Classification Report on Test Set:")
print(classification_report(y_test, y_pred))

"""## Final Model Selection

After testing multiple models and strategies (feature selection, SMOTE, grid search), the best performing model is:

**XGBoost (with selected features)**  
→ Recall (class 1): 0.84  
→ F1-score (class 1): 0.64  
→ Accuracy: 0.86

This model achieves the best balance between precision and recall while remaining robust and interpretable.
"""